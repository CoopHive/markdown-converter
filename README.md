# ğŸ§  CoopHive: Modular RAG Pipelines for Scientific Papers

CoopHive builds modular, reproducible RAG (Retrieval-Augmented Generation) pipelines for scientific literature. It supports customizable document processing, embedding, querying, and incentivization mechanisms, enabling decentralized and transparent scientific collaboration.

---

## âœ¨ Features

- **Modular Architecture**: Easily swap converters, chunkers, embedders, and reward strategies via configuration files. Each component (conversion, chunking, embedding) is pluggable, allowing you to add your own custom methods or extend existing ones.
- **Reproducibility**: Deterministic pipelines with version-controlled configs ensure consistent, repeatable results.
- **Transparency**: All processing and contributions are traceable through Git commits and IPFS hashes.
- **Built-in Pipelines**:
  - Document conversion to markdown
  - Text chunking
  - Embedding of chunks
  - Storage in vector databases and decentralized storage
  - Database recreation from decentralized storage
  - Querying and evaluation of vector databases
  - Token rewarding for contributors

Examples for common methods (like Marker, OpenAI, Lighthouse, etc.) are provided but can be extended with any custom logic by implementing your own classes.

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- Poetry
- Node.js 18+
- Access to APIs (e.g., OpenAI, Lighthouse, OpenRouter)

### Installation

```bash
git clone https://github.com/your-repo/coophive-markdown-converter.git
cd coophive-markdown-converter
bash scripts/setup.sh
poetry lock --no-update
poetry install
cp .env.example .env
```

### ğŸ’¡ Environment Variables

Create a `.env` file with the following keys:

```bash
OPENAI_API_KEY=
NEO4J_URI=
NEO4J_USER=
NEO4J_PASSWORD=
OWNER_ADDRESS=
PRIVATE_KEY=
LIGHTHOUSE_TOKEN=
OPENROUTER_API_KEY=
```

### Running Modules

```bash
bash scripts/run_processor.sh         # Convert, chunk, embed, and store documents
bash scripts/run_db_creator.sh         # Recreate DBs from IPFS graph
bash scripts/run_evaluation.sh         # Query and evaluate across DBs
bash scripts/run_token_reward.sh       # Distribute ERC20 token rewards
```

Or launch an interactive session:

```bash
poetry shell
```

### Code Quality and Testing

```bash
bash scripts/lint.sh                   # Lint (black, isort, flake8, mypy)
bash scripts/test.sh --integration      # Run integration tests
```

---

## ï¿½\uddna Module Configuration

### âœ¨ Processor

- Converts PDF to markdown using a configurable converter module
- Chunks text into paragraphs, sentences, or fixed lengths
- Embeds text chunks using a configurable embedder
- Uploads outputs to IPFS and stores metadata in databases
- Logs all processing as Git commits

All core components are modular and can be customized by implementing new classes under `descidb/core/` and specifying them in the config file.

Configurable at [`config/processor.yml`](config/processor.yml):

```yaml
converter: marker # Options: marker, openai, custom
chunker: paragraph # Options: paragraph, sentence, fixed_length, custom
embedder: openai # Options: openai, nvidia, custom
```

### ğŸ” DB Creator

- Traverses the IPFS graph in Neo4j
- Rebuilds documents, chunks, and embeddings
- Supports depth control and relationship mapping

Configurable at [`config/db_creator.yml`](config/db_creator.yml):

```yaml
components:
  converter:
    - marker
  chunker:
    - fixed_length
  embedder:
    - openai

cids_file_paths:
  - cids.txt
```

### ğŸ” Evaluation Agent

- Runs queries across vector DBs
- Uses an LLM to rank and analyze retrieval performance

Example snippet:

```yaml
query: "impact of CRISPR on neuroscience"
model_name: "gpt-4"
```

Outputs are saved in `temp/evaluation/`.

### ğŸ… Token Rewarding

- Analyzes contributions and distributes ERC20 tokens
- Supports job count, bonuses, and time decay reward models

Configuration example:

```yaml
databases:
  - converter: openai
    chunker: paragraph
    embedder: openai
```

Extend `descidb/rewards/token_rewarder.py` to customize reward mechanisms.

---

## ğŸ› ï¸ Tech Stack

- **Python** (workflow orchestration)
- **ChromaDB** (vector storage)
- **Neo4j** (graph lineage and job tracking)
- **IPFS / Lighthouse** (document storage)
- **Embedding Models** (pluggable, default examples included)
- **Hardhat + Solidity** (ERC20 token management)
- **Docker + Nomad** (optional deployment)

---

## ğŸ“„ Directory Overview

```bash
coophive-markdown-converter/
â”œâ”€â”€ config/        # YAML pipeline configs
â”œâ”€â”€ descidb/       # Core libraries (processing, storage, rewards)
â”œâ”€â”€ scripts/       # CLI scripts for pipelines
â”œâ”€â”€ contracts/     # Blockchain contract ABIs
â”œâ”€â”€ docker/        # Container specs
â”œâ”€â”€ erc20-token/   # Token contract config
â”œâ”€â”€ papers/        # Example documents
â”œâ”€â”€ tests/         # Unit and integration tests
â””â”€â”€ .github/       # CI/CD configurations
```

---

## ğŸ“… License

This project is open-sourced under the MIT License.
